# -*- coding: utf-8 -*-
"""linear_associator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13dq7r3l9z8huhmCIgeBqQEEvA7Mh_xR8
"""

# Bhavanam Honnappa Gowda, Anil 
# 1001_966_285
# 2022_10_09
# Assignment_02_01

import numpy as np


class LinearAssociator(object):
    def __init__(self, input_dimensions=2, number_of_nodes=4, transfer_function="Hard_limit"):
       
        self.input_Dim,self.n_of_Nodes,self.transFunc = input_dimensions,number_of_nodes,transfer_function
        self.wghts = np.array([],[])
        self.initialize_weights()

    def initialize_weights(self, seed=None):
       
        if seed == None: seed = None
        else:
            seed = np.random.seed(seed)
        wt = np.random.randn(self.n_of_Nodes, self.input_Dim)
        self.wghts = wt

    def set_weights(self, W):
      
        weight = np.random.randn(self.n_of_Nodes, self.input_Dim)
        if (W.shape == weight.shape): self.wghts = W
        else:
            return -1


    def get_weights(self):
        
        return self.wghts
    def predict(self, X):
     
        net = self.wghts@ X
        if self.transFunc == 'Hard_limit': actual_value = (net >= 0).astype(int)
        elif self.transFunc == 'Linear': actual_value = net
        return actual_value


    def fit_pseudo_inverse(self, X, y):
      
        if X.shape[0] < X.shape[1]:
            X = X.T
            XP = np.linalg.inv(X.T@ X) @ X.T
            self.wghts = y@ XP.T
        else:
            XP = np.linalg.inv(X.T@ X)@ X.T
            self.wghts = y@ XP

    def train(self, X, y, batch_size=5, num_epochs=50, alpha=0.4, gamma=0.9, learning="Delta"):
       
        X_split,y_split = np.array_split(X, batch_size, 1), np.array_split(y, batch_size, 1)
        for i in range(num_epochs):
            for b in range(batch_size):
                actual_value = self.predict(X_split[b])
                if learning.lower == 'unsupervised_hebb':
                    self.wghts = self.wghts + alpha * np.dot(actual_value, X_split[b].T)
                elif learning.lower() == 'delta':
                     error = y_split[b] - actual_value
                     self.wghts = self.wghts +  alpha * np.dot(error, X_split[b].T)
                elif learning.lower == 'filtered':
                    self.wghts = self.wghts + alpha * np.dot(y_split[b], X_split[b].T)
                    self.wghts = (1 - gamma) * self.wghts


    def calculate_mean_squared_error(self, X, y):
     
        actual_value = self.predict(X)
        error = y - actual_value
        mean_squared_error = np.mean(error ** 2)
        return mean_squared_error



